{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:27:56.652970Z",
     "start_time": "2025-12-26T10:27:56.650972Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "26f630d3009cdcd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T10:36:58.945556Z",
     "start_time": "2025-12-26T10:27:56.705182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Input, Conv2D, MaxPooling2D, Dropout, Rescaling\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# --- 1. DATA LOADING (SPARSE / INTEGER MODE) ---\n",
    "# This mode is safer and fixes the Shape Mismatch error\n",
    "TARGET_SIZE = (224, 224)\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "BATCHSIZE = 32\n",
    "\n",
    "# Load Training Data (Int Mode)\n",
    "train_raw = image_dataset_from_directory(\n",
    "    './train_224/',\n",
    "    image_size=TARGET_SIZE,\n",
    "    batch_size=BATCHSIZE,\n",
    "    label_mode='int',  # <--- Changed to 'int' to match your error shape (None, 1)\n",
    "    shuffle=True,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Load Validation Data (Int Mode)\n",
    "val_raw = image_dataset_from_directory(\n",
    "    './test_224/',\n",
    "    image_size=TARGET_SIZE,\n",
    "    batch_size=BATCHSIZE,\n",
    "    label_mode='int',  # <--- Changed to 'int'\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train_raw.class_names\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(f\"Classes: {class_names} (Total: {NUM_CLASSES})\")\n",
    "\n",
    "# Normalization\n",
    "normalization_layer = Rescaling(1./255)\n",
    "train_ds = train_raw.map(lambda x, y: (normalization_layer(x), y)).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(lambda x, y: (normalization_layer(x), y)).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# --- 2. CALLBACKS ---\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('accuracy'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_accuracy'))\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('accuracy'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_accuracy'))\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        if loss_type == 'epoch':\n",
    "            plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "            plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.show()\n",
    "\n",
    "history_this = LossHistory()\n",
    "\n",
    "# --- 3. MODELS (COMPATIBLE WITH INTEGERS) ---\n",
    "\n",
    "def vgg16(num_class, epochs=20, frozen=15, lr=0.001, patience=2, dropout_rate=0.5, verbose=1, savepath='./VGG16.h5', history=history_this, input_shape=INPUT_SHAPE):\n",
    "    # Check for weights file\n",
    "    weights_path = './vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    if os.path.exists(weights_path):\n",
    "        base_model = VGG16(include_top=False, weights=weights_path, input_shape=input_shape)\n",
    "    else:\n",
    "        # Fallback to download if local file missing\n",
    "        print(\"Local weights not found, downloading...\")\n",
    "        base_model = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "    for layer in base_model.layers[:frozen]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[frozen:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(num_class, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions, name='vgg16')\n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    # CRITICAL FIX: Using 'sparse_categorical_crossentropy' handles (None, 1) targets automatically\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    earlyStopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=patience, verbose=verbose, mode='max')\n",
    "    saveBestModel = callbacks.ModelCheckpoint(filepath=savepath, monitor='val_accuracy', verbose=verbose, save_best_only=True, mode='max')\n",
    "\n",
    "    hist = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[earlyStopping, saveBestModel, history],\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return hist\n",
    "\n",
    "# --- 4. PREDICTION ---\n",
    "def prediction(model_path):\n",
    "    model = load_model(model_path)\n",
    "    rootdir = './test_224/'\n",
    "    test_labels = []\n",
    "    test_images = []\n",
    "\n",
    "    # Walk directories\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith((\".jpeg\", \".jpg\", \".png\")):\n",
    "                label_name = subdir.split('/')[-1]\n",
    "                test_labels.append(label_name)\n",
    "                test_images.append(os.path.join(subdir, file))\n",
    "\n",
    "    predict_results = []\n",
    "    class_indices = {name: i for i, name in enumerate(class_names)}\n",
    "    index_to_label = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "    print(f\"Predicting {len(test_images)} images...\")\n",
    "\n",
    "    for img_path in test_images:\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            img = img.resize(TARGET_SIZE)\n",
    "            img_array = np.array(img) / 255.0  # Normalize\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "            # Predict\n",
    "            pred_probs = model.predict(img_array, verbose=0)\n",
    "            pred_class_id = np.argmax(pred_probs, axis=1)[0] # Returns integer index\n",
    "\n",
    "            predict_results.append(index_to_label[pred_class_id])\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {img_path}: {e}\")\n",
    "            predict_results.append(\"error\")\n",
    "\n",
    "    return accuracy_score(test_labels, predict_results)\n",
    "\n",
    "# --- 5. HYPEROPT ---\n",
    "def objective(params):\n",
    "    print(f\"Testing: {params}\")\n",
    "    try:\n",
    "        vgg16(num_class=NUM_CLASSES,\n",
    "              frozen=int(params['frozen']),\n",
    "              epochs=int(params['epochs']),\n",
    "              patience=int(params['patience']),\n",
    "              lr=params['lr'],\n",
    "              dropout_rate=params['dropout_rate'],\n",
    "              verbose=0)\n",
    "\n",
    "        acc = prediction('./VGG16.h5')\n",
    "        print(f'Accuracy: {acc}')\n",
    "        return {'loss': -acc, 'status': STATUS_OK}\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return {'loss': 0, 'status': STATUS_OK} # Fail gracefully\n",
    "\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 15, 18, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 8, 1),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.loguniform('lr', np.log(0.0001), np.log(0.01)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.3, 0.6),\n",
    "}\n",
    "\n",
    "# Run\n",
    "t1 = time.time()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=2)\n",
    "print(f\"Best: {best}\")"
   ],
   "id": "c058a2792bb723ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "Found 287 files belonging to 5 classes.\n",
      "Found 75 files belonging to 4 classes.\n",
      "Classes: ['0', '1', '2', '3', '4'] (Total: 5)\n",
      "Testing: {'dropout_rate': 0.4829415228923738, 'epochs': 7.0, 'frozen': 18.0, 'lr': 0.0016010353790399281, 'patience': 3.0}\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 75 images...                              \n",
      "Accuracy: 1.0                                        \n",
      "Testing: {'dropout_rate': 0.4978189394165963, 'epochs': 5.0, 'frozen': 16.0, 'lr': 0.00011515140064929637, 'patience': 2.0}\n",
      " 50%|█████     | 1/2 [05:26<05:26, 326.37s/trial, best loss: -1.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 75 images...                                           \n",
      "Accuracy: 1.0                                                     \n",
      "100%|██████████| 2/2 [09:02<00:00, 271.08s/trial, best loss: -1.0]\n",
      "Best: {'dropout_rate': np.float64(0.4829415228923738), 'epochs': np.float64(7.0), 'frozen': np.float64(18.0), 'lr': np.float64(0.0016010353790399281), 'patience': np.float64(3.0)}\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
